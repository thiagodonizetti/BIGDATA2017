{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# ** Análise Exploratória **\n",
    "\n",
    "#### Esse notebook introduz os conceitos de Análise Exploratória\n",
    "\n",
    "#### Para isso utilizaremos a base de dados de [Crimes de São Francisco](https://www.kaggle.com/c/sf-crime) obtidos do site de competições [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "#### ** Esse notebook contém:  **\n",
    "#### *Parte 1:* *Parsing* da base de dados de Crimes de São Francisco\n",
    "#### *Parte 2:* Estatísticas Básicas das Variáveis\n",
    "#### *Parte 3:* Plotagem de Gráficos\n",
    "\n",
    "#### Para os exercícios é aconselhável consultar a documentação da [API do PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Parte 1: Parsing da Base de Dados **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa primeira parte do notebook vamos aprender a trabalhar com arquivos CSV. Os arquivos CSV são arquivos textos representando tabelas de dados, numéricas ou categóricas, com formatação apropriada para a leitura estruturada.\n",
    "\n",
    "#### A primeira linha de um arquivo CSV é o cabeçalho, com o nome de cada coluna da tabela separados por vírgulas.\n",
    "\n",
    "#### Cada linha subsequente representa um objeto da base de dados com os valores também separados por vírgula. Esses valores podem ser numéricos, categóricos (textuais) e listas. As listas são representadas por listas de valores separadas por vírgulas e entre aspas.\n",
    "\n",
    "#### Vamos carregar a base de dados histórica de Crimes de São Francisco, um dos temas do projeto final. No primeiro passo vamos armazenar o cabeçalho em uma variável chamada `header` e imprimi-la para a descrição dos campos de nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campos disponíveis: Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filename = os.path.join(\"Data\",\"Aula03\",\"Crime.csv\")\n",
    "CrimeRDD = sc.textFile(filename,8)\n",
    "header = CrimeRDD.take(1)[0] # o cabeçalho é a primeira linha do arquivo\n",
    "\n",
    "print \"Campos disponíveis: {}\".format(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durante os exercícios precisaremos pular a linha do cabeçalho de tal forma a trabalhar apenas com a tabela de dados.\n",
    "\n",
    "#### Uma forma de fazer isso é utilizando o comando `filter()` para eliminar toda linha igual a variável `header`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "CrimeHeadlessRDD = CrimeRDD.filter(lambda x: x != header)\n",
    "\n",
    "firstObject = CrimeHeadlessRDD.take(1)[0]\n",
    "print firstObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObject=='2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747', 'valor incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora temos um dataset em que cada linha é uma string contendo todos os valores. Porém, para explorarmos os dados precisamos que cada objeto seja uma lista de valores.\n",
    "\n",
    "#### Utilize o comando `split()` para transformar os objetos em listas de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "CrimeHeadlessRDD = (CrimeRDD\n",
    "                    .filter(lambda x: x != header)\n",
    "                    .map(lambda x: x.split(\",\"))\n",
    "                    )\n",
    "\n",
    "firstObjectList = CrimeHeadlessRDD.take(1)[0]\n",
    "print firstObjectList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObjectList[0]==u'2015-05-13 23:53:00', 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime(Dates=datetime.datetime(2015, 5, 13, 23, 53), Category=u'WARRANTS', Descript=u'WARRANT ARREST', DayOfWeek=u'Wednesday', PdDistrict=u'NORTHERN', Resolution=[u'ARREST, BOOKED'], Address=u'OAK ST / LAGUNA ST', COORD=(u'-122.425891675136', u'37.7745985956747'))\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "headeritems = header.split(',') # transformar o cabeçalho em lista\n",
    "del headeritems[-1] # apagar o último item e...\n",
    "headeritems[-1] = 'COORD' # transformar em COORD\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "Crime = namedtuple('Crime',headeritems) # gera a namedtuple Crime com os campos de header\n",
    "\n",
    "REGEX = r',(?=(?:[^\"]*\"[^\"]*\")*(?![^\"]*\"))'\n",
    "# buscar por \",\" tal que após essa vírgula (?=) ou exista um par de \"\" ou não tenha \" sozinha\n",
    "# ?= indica para procurarmos pelo padrão após a vírgula\n",
    "# ?: significa para não interpretar os parênteses como captura de valores\n",
    "# [^\"]* 0 ou sequências de caracteres que não sejam aspas\n",
    "# [^\"]*\"[^\"]*\"  <qualquer caracter exceto aspas> \" <qualquer caracter exceto aspas> \"\n",
    "# ?! indica para verificar se não existe tal padrão a frente da vírgula\n",
    "\n",
    "\n",
    "def ParseCrime(rec):\n",
    "    # utilizando re.split() vamos capturar nossos valores\n",
    "    Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, X, Y = re.split(REGEX,rec)\n",
    "    \n",
    "    # Converta a data para o formato datetime\n",
    "    Date = datetime.datetime.strptime(Date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # COORD é uma tupla com floats representando X e Y\n",
    "    COORD = (X, Y)\n",
    "    \n",
    "    # O campos 'Resolution' será uma lista dos valores separados por vírgula, sem as aspas\n",
    "    Resolution = [ Resolution.replace('\"', '') ] \n",
    "    return Crime(Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, COORD)\n",
    "\n",
    "# Aplique a função ParseCrime para cada objeto da base\n",
    "CrimeHeadlessRDD = (CrimeRDD\n",
    "                    .filter(lambda x: x != header)\n",
    "                    .map(lambda x: ParseCrime(x)) \n",
    "                    )\n",
    "\n",
    "firstClean = CrimeHeadlessRDD.take(1)[0]\n",
    "totalRecs = CrimeHeadlessRDD.count()\n",
    "print firstClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparem que o campo *Resolution* cujo valor no primeiro registro era \"ARREST, BOOKED\" se tornou dois campos diferentes por causa do `split()`.\n",
    "\n",
    "#### Nesses casos em que uma simples separação não funciona, nós podemos utilizar as [Expressões Regulares](http://www.rexegg.com/regex-quickstart.html). O Python tem suporte as Regex através da biblioteca `re`. Vamos utilizar o comando [`re.split()`](https://docs.python.org/2/library/re.html#re.split) para cuidar da separação de nossa base em campos.\n",
    "\n",
    "#### Além disso, vamos aproveitar para converter o primeiro campo, que representa data e hora, para objeto do tipo [`datetime`](https://docs.python.org/2/library/datetime.html) através do comando `datetime.datetime.strptime()`. Também vamos agrupar as coordenadas X e Y em uma tupla de floats.\n",
    "\n",
    "#### Outra ajuda que o Python pode nos dar é a utilização das [`namedtuple`](https://docs.python.org/2/library/collections.html#namedtuple-factory-function-for-tuples-with-named-fields) que permite acessar cada campo de cada objeto pelo nome. Ex.: rec.Dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert type(firstClean.Dates) is datetime.datetime and type(firstClean.Resolution) is list and type(firstClean.COORD) is tuple,'tipos incorretos'\n",
    "print \"OK\"\n",
    "\n",
    "assert CrimeHeadlessRDD.filter(lambda x: len(x)!=8).count()==0, 'algo deu errado!'\n",
    "print \"OK\"\n",
    "\n",
    "assert totalRecs==878049, 'total de registros incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 2: Estatísticas Básicas das Variáveis **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a filtrar a base de dados para calcular estatísticas básicas necessárias para a análise exploratória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2a) Contagem de frequência **\n",
    "\n",
    "A contagem de frequência é realizada de forma similar ao exercício de contagem de palavras. Primeiro mapeamos a variável de interesse. Como exemplo vamos gerar uma lista da quantidade total de cada tipo de crime (Category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'LARCENY/THEFT', 174900), (u'OTHER OFFENSES', 126182), (u'NON-CRIMINAL', 92304), (u'ASSAULT', 76876), (u'DRUG/NARCOTIC', 53971), (u'VEHICLE THEFT', 53781), (u'VANDALISM', 44725), (u'WARRANTS', 42214), (u'BURGLARY', 36755), (u'SUSPICIOUS OCC', 31414), (u'MISSING PERSON', 25989), (u'ROBBERY', 23000), (u'FRAUD', 16679), (u'FORGERY/COUNTERFEITING', 10609), (u'SECONDARY CODES', 9985), (u'WEAPON LAWS', 8555), (u'PROSTITUTION', 7484), (u'TRESPASS', 7326), (u'STOLEN PROPERTY', 4540), (u'SEX OFFENSES FORCIBLE', 4388), (u'DISORDERLY CONDUCT', 4320), (u'DRUNKENNESS', 4280), (u'RECOVERED VEHICLE', 3138), (u'KIDNAPPING', 2341), (u'DRIVING UNDER THE INFLUENCE', 2268), (u'RUNAWAY', 1946), (u'LIQUOR LAWS', 1903), (u'ARSON', 1513), (u'LOITERING', 1225), (u'EMBEZZLEMENT', 1166), (u'SUICIDE', 508), (u'FAMILY OFFENSES', 491), (u'BAD CHECKS', 406), (u'BRIBERY', 289), (u'EXTORTION', 256), (u'SEX OFFENSES NON FORCIBLE', 148), (u'GAMBLING', 146), (u'PORNOGRAPHY/OBSCENE MAT', 22), (u'TREA', 6)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "CatCountRDD = (CrimeHeadlessRDD\n",
    "               .map(lambda x: (x.Category,1))\n",
    "               .reduceByKey(lambda x,y: x+y)\n",
    "               )\n",
    "catCount = sorted(CatCountRDD.collect(), key=lambda x: -x[1])\n",
    "print catCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert catCount[0][1]==174900, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar, vamos gerar a contagem para as regiões de São Francisco (PdDistrict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'SOUTHERN', 157182), (u'MISSION', 119908), (u'NORTHERN', 105296), (u'BAYVIEW', 89431), (u'CENTRAL', 85460), (u'TENDERLOIN', 81809), (u'INGLESIDE', 78845), (u'TARAVAL', 65596), (u'PARK', 49313), (u'RICHMOND', 45209)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "RegionCountRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda x: (x.PdDistrict,1))\n",
    "                 .reduceByKey(lambda x,y: x+y)\n",
    "                 )\n",
    "regCount = sorted(RegionCountRDD.collect(), key=lambda x: -x[1])\n",
    "print regCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert regCount[0][1]==157182, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Cálculo da Média**\n",
    "\n",
    "#### Nesse exercício vamos calcular a média de crimes em cada região para cada dia da semana. Para isso, primeiro devemos calcular a quantidade de dias de cada dia da semana que existem na base de dados, para isso vamos criar uma RDD de tuplas em que o primeiro campo é a tupla da data no formato 'dia-mes-ano' e do dia da semana e o segundo campo o valor $1$.\n",
    "\n",
    "#### Em seguida, reduzimos a RDD sem efetuar a soma, mantendo o valor $1$. Essa redução filtra a RDD para que cada data apareça uma única vez. Ao final,  podemos efetuar o mapeamento de (DayOfWeek,1) e redução com soma para contabilizar quantas vezes cada dia da semana aparece na base de dados.\n",
    "\n",
    "#### Nossa próxima RDD terá como chave uma tupla ( (DayOfWeek, PdDistrict), 1) para contabilizar quantos crimes ocorreram em determinada região e naquele dia da semana. Após a redução, devemos mapear esse RDD para (DayOfWeek, (PdDistrict, contagem)).\n",
    "\n",
    "#### Finalmente, podemos juntar as duas RDDs uma vez que elas possuem a mesma chave (DayOfWeek), dessa forma teremos tuplas no formato ( DayOfWeek, ( (PdDistrict,contagem), contagemDiaDaSemana ) ). Isso deve ser mapeado para:\n",
    "\n",
    "#### ( DayOfWeek, ( PdDistrict, contagem / contagemDiaDaSemana ) )\n",
    "\n",
    "#### Lembrando de converter `contagemDiaDaSemana` para `float`. Finalmente, o resultado pode ser agrupado pela chave, gerando uma tupla ( DayOfWeek, [ (Pd1, media1), (Pd2, media2), ... ] ). Essa lista pode ser mapeada para um dicionário com o comando `dict`.\n",
    "\n",
    "#### No final, transformamos o RDD em um dicionário Python com o comando `collectAsMap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'CENTRAL': 37.99688473520249, u'NORTHERN': 44.45794392523364, u'SOUTHERN': 64.82866043613707, u'PARK': 20.70404984423676, u'MISSION': 49.45171339563863, u'TENDERLOIN': 31.707165109034268, u'RICHMOND': 18.968847352024923, u'TARAVAL': 25.953271028037385, u'INGLESIDE': 32.230529595015575, u'BAYVIEW': 37.27414330218068}\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "from operator import add\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda x: ((day2str(x.Dates), x.DayOfWeek), 1))\n",
    "                 .reduceByKey(lambda x,y: (x))\n",
    "                 .map(lambda (x,y): (x[1], 1))\n",
    "                 .reduceByKey(lambda x,y: x+y)\n",
    "                 )\n",
    "\n",
    "#print totalDatesRDD.take(10)\n",
    "\n",
    "crimesWeekDayRegionRDD = (CrimeHeadlessRDD\n",
    "                           .map(lambda x: ( (x.DayOfWeek, x.PdDistrict), 1 ))\n",
    "                           .reduceByKey(lambda x,y: x+y )\n",
    "                           .map(lambda (x,y): (x[0], (x[1], y)))                        \n",
    "                          )\n",
    "\n",
    "#print crimesWeekDayRegionRDD.take(2)\n",
    "\n",
    "RegionAvgPerDayRDD = (crimesWeekDayRegionRDD\n",
    "                      .join(totalDatesRDD)\n",
    "                      .map(lambda (x,(y,z)): (x, (y[0], y[1]/ float(z))))\n",
    "                      .groupByKey()\n",
    "                      .mapValues(dict)\n",
    "                     )\n",
    "#print RegionAvgPerDayRDD.take(2)\n",
    "RegionAvg = RegionAvgPerDayRDD.collectAsMap()\n",
    "print RegionAvg['Sunday']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert np.round(RegionAvg['Sunday']['BAYVIEW'],2)==37.27, 'valores incorretos {}'.format(np.round(RegionAvg[0][2],2))\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Média e Desvio-Padrão pelo PySpark **\n",
    "\n",
    "#### Uma alternativa para calcular média, desvio-padrão e outros valores descritivos é utilizando os comandos internos do Spark. Para isso é necessário gerar uma RDD de listas de valores.\n",
    "\n",
    "#### Gere uma RDD contendo a tupla ( (Dates,DayOfWeek, PdDistrict), contagem), mapeie para ( (DayOfWeek,PdDistrict), Contagem) e agrupe pela chave. Isso irá gerar uma RDD ( (DayOfWeek,PdDistrict), Iterador(contagens) ).\n",
    "\n",
    "#### Agora crie um dicionário RegionAvgSpark, inicialmente vazio e colete apenas o primeiro elemento da tupla para a variável `Keys`. Itere essa variável realizando os seguintes passos:\n",
    "\n",
    "* #### Se `key[0]` não existir no dicionário, crie a entrada `key[0]` como um dicionário vazio.\n",
    "* #### Mapeie countWeekDayDistRDD filtrando por `key` e gerando a RDD com os valores da tupla. Note que não queremos uma lista de listas.\n",
    "* #### Insira a tupla (media, desvio-padrão) utilizando os comandos `mean()` e `stdev()` do PySpark, armazenando na chave RegionAvgSpark[ key[0] ][ key[1] ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "countWeekDayDistRDD = (CrimeHeadlessRDD\n",
    "                       .map(lambda x: ((x.Dates, x.DayOfWeek, x.PdDistrict), 1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .map(lambda (x,y): ((x[1], x[2]), y))\n",
    "                       .groupByKey()                       \n",
    "                       )\n",
    "\n",
    "\n",
    "\n",
    "# Esse procedimento só é viável se existirem poucas chaves\n",
    "RegionAvgSpark = {}\n",
    "Keys = countWeekDayDistRDD.map(lambda rec: rec[0]).collect()\n",
    "for key in Keys:\n",
    "    listRDD = (countWeekDayDistRDD\n",
    "               .filter(lambda rec: rec[0]==key)\n",
    "               .flatMap(lambda rec: rec[1])\n",
    "               )\n",
    "    if key[0] not in RegionAvgSpark:\n",
    "        RegionAvgSpark[key[0]] = {}    \n",
    "    RegionAvgSpark[key[0]][key[1]] = (listRDD.mean(), listRDD.stdev())\n",
    "    \n",
    "print RegionAvgSpark['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.round(RegionAvgSpark['Sunday']['BAYVIEW'][0],2)==37.39 and np.round(RegionAvgSpark['Sunday']['BAYVIEW'][1],2)==10.06, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: Plotagem de Gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a manipular os dados para gerar listas de valores a serem utilizados na plotagem de gráficos.\n",
    "\n",
    "#### Para a plotagem de gráficos vamos utilizar o [`matplotlib`](http://matplotlib.org/) que já vem por padrão na maioria das distribuições do Python (ex.: Anaconda). Outras bibliotecas alternativas interessantes são: [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) e [Bokeh](http://bokeh.pydata.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Gráfico de Barras **\n",
    "\n",
    "#### O gráfico de barras é utilizado quando queremos comparar dados entre categorias diferentes de uma variável categórica. Como exemplo, vamos contabilizar o número médio de crimes diários por região.\n",
    "\n",
    "#### Vamos primeiro criar a RDD totalDatesRDD que contém a lista de dias únicos, computaremos o total de dias com o comando `count()` armazenando na variável `totalDays`. Não se esqueça de converter o valor para `float`.\n",
    "\n",
    "#### Em seguida, crie o RDD avgCrimesRegionRDD que utiliza a RDD RegionCountRDD para calcular a média de crimes por região.\n",
    "\n",
    "#### Utilizando o comando `zip()` do Python é possível descompactar um dicionário em duas variáveis, uma com as chaves e outra com os valores. Utilizaremos essas variáveis para a plotagem do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda rec: (day2str(rec.Dates),1))\n",
    "                 .reduceByKey(lambda x,y: x)\n",
    "                 )\n",
    "\n",
    "totalDays = float(totalDatesRDD.count())\n",
    "\n",
    "avgCrimesRegionRDD = (RegionCountRDD\n",
    "                      .map(lambda rec: (rec[0],rec[1]/totalDays))\n",
    "                     )\n",
    "\n",
    "Xticks,Y = zip(*avgCrimesRegionRDD.collectAsMap().items())\n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.bar(indices,Y, width)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quando temos subcategorias de interesse, podemos plotar através de um gráfico de barras empilhado. Vamos plotar o conteúdo da variável RegionAvg.\n",
    "\n",
    "#### Primeiro passo é criar um dicionário `Y` em que a chave é o dia da semana e o valor é uma `np.array` contendo a média de cada região para aquele dia.\n",
    "\n",
    "#### Em seguida precisamos criar uma matriz `Bottom` que determina qual é o início de cada uma das barras. O início da barra do dia `i` deve ser o final da barra do dia `i-1`.\n",
    "\n",
    "#### Com isso calculado podemos gerar um plot por dia com o parâmetro bottom correspondente ao vetor `Bottom` daquele dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dias da semana como referência\n",
    "Day = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "# Uma cor para cada dia\n",
    "Color = ['r','b','g','y','c','k','purple']\n",
    "\n",
    "# Dicionário (dia, array de médias)\n",
    "Y = {}\n",
    "for day in Day:\n",
    "    Y[day] = np.array([RegionAvg[day][x] for x in Xticks])\n",
    "\n",
    "# Matriz dias x regiões    \n",
    "Bottom = np.zeros( (len(Day),len(Xticks)) )\n",
    "for i in range(1,len(Day)):\n",
    "    Bottom[i,:] = Bottom[i-1,:]+Y[Day[i-1]]\n",
    "    \n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "\n",
    "# Gera uma lista de plots, um para cada dia\n",
    "plots = [plt.bar(indices,Y[Day[i]], width, color=Color[i], bottom=Bottom[i]) for i in range(len(Day))]\n",
    "\n",
    "plt.legend( [p[0] for p in plots], Day,loc='center left', bbox_to_anchor=(1, 0.5) ) \n",
    "    \n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Gráfico de Linha **\n",
    "\n",
    "#### O gráfico de linha é utilizado principalmente para mostrar uma tendência temporal.\n",
    "\n",
    "#### Nesse exercício vamos primeiro gerar o número médio de crimes em cada hora do dia.\n",
    "\n",
    "#### Primeiro, novamente, geramos um RDD contendo um único registro de cada hora para cada dia. Em seguida, contabilizamos a soma da quantidade de crime em cada hora. Finalmente, juntamos as duas RDDs e calculamos a média dos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2248), (8, 2249), (16, 2248), (1, 2247), (17, 2248), (9, 2248), (2, 2239), (18, 2248), (10, 2248), (11, 2248)]\n",
      "[(0, 19.95774021352313), (8, 14.62872387727879)]\n",
      "[(0, 19.95774021352313), (8, 14.62872387727879), (16, 22.302935943060497), (1, 11.64797507788162), (17, 23.822508896797153)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseWeekday = lambda x: '{}-{}-{}'.format(x.day, x.month, x.year)\n",
    "\n",
    "\n",
    "hoursRDD = (CrimeHeadlessRDD\n",
    "            .map(lambda x: ((parseWeekday(x.Dates), x.Dates.hour), 1))\n",
    "            .reduceByKey(lambda x,y: x)\n",
    "            .map(lambda (x,y): (x[1], 1))\n",
    "            .reduceByKey(lambda x,y: x+y)\n",
    "           )\n",
    "print hoursRDD.take(10)\n",
    "\n",
    "crimePerHourRDD = (CrimeHeadlessRDD\n",
    "                   .map(lambda x: (x.Dates.hour, 1))\n",
    "                   .reduceByKey(lambda x,y: x+y)\n",
    "                  )\n",
    "\n",
    "avgCrimeHourRDD = (crimePerHourRDD\n",
    "                   .join(hoursRDD)\n",
    "                   .map(lambda (x,y): (x, float (float(y[0]) / float(y[1]))))\n",
    "                  )\n",
    "\n",
    "print avgCrimeHourRDD.take(2)\n",
    "crimePerHour = avgCrimeHourRDD.collect()\n",
    "print crimePerHour[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert np.round(crimePerHour[0][1],2)==19.96, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crimePerHourSort = sorted(crimePerHour,key=lambda x: x[0])\n",
    "\n",
    "X,Y = zip(*crimePerHourSort)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.plot(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('Avg. Number of crimes')\n",
    "plt.xlabel('Hour')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Gráfico de Dispersão**\n",
    "\n",
    "#### O gráfico de dispersão é utilizado para visualizar correlações entre as variáveis. Com esse gráfico é possível observar se o crescimento da quantidade de uma categoria está relacionada ao crescimento/decrescimento de outra (mas não podemos dizer se uma causa a outra).\n",
    "\n",
    "#### Na primeira parte do exercício calcularemos a correlação entre os diferentes tipos de crime. Para isso primeiro precisamos construir uma RDD em que cada registro corresponde a uma data o valor contido nele é a quantidade de crimes de cada tipo.\n",
    "\n",
    "#### Diferente dos exercícios anteriores, devemos manter essa informação como uma lista de valores em que todos os registros sigam a mesma ordem da lista de crimes.\n",
    "\n",
    "#### O primeiro passo é criar uma RDD com a tupla ( (Mes-Ano, Crime), 1 ) e utilizá-la para gerar a tupla ( (Mes-Ano,Crime) Quantidade ).\n",
    "\n",
    "#### Mapeamos essa RDD para definir Mes-Ano como chave e agrupamos em torno dessa chave, gerando uma lista de quantidade de crimes em cada data. Aplicamos a função `dict()` nessa lista para obtermos uma RDD no seguinte formato: (Mes-Ano, {CRIME: quantidade}).\n",
    "\n",
    "#### Além disso, vamos criar a variável `crimes` contendo a lista de crimes contidas na lista de pares `catCount` computada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2-2007', {u'KIDNAPPING': 12, u'WEAPON LAWS': 49, u'SECONDARY CODES': 61, u'WARRANTS': 296, u'LOITERING': 4, u'EMBEZZLEMENT': 13, u'FRAUD': 87, u'DRIVING UNDER THE INFLUENCE': 13, u'VEHICLE THEFT': 212, u'ROBBERY': 135, u'BURGLARY': 192, u'SUSPICIOUS OCC': 207, u'RECOVERED VEHICLE': 30, u'ASSAULT': 456, u'FORGERY/COUNTERFEITING': 81, u'BAD CHECKS': 3, u'DRUNKENNESS': 24, u'GAMBLING': 2, u'OTHER OFFENSES': 757, u'SUICIDE': 3, u'ARSON': 5, u'DRUG/NARCOTIC': 504, u'SEX OFFENSES NON FORCIBLE': 2, u'PROSTITUTION': 76, u'VANDALISM': 237, u'NON-CRIMINAL': 490, u'LIQUOR LAWS': 17, u'TRESPASS': 50, u'SEX OFFENSES FORCIBLE': 23, u'STOLEN PROPERTY': 19, u'BRIBERY': 3, u'FAMILY OFFENSES': 4, u'MISSING PERSON': 154, u'DISORDERLY CONDUCT': 24, u'RUNAWAY': 13, u'LARCENY/THEFT': 835})]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "parseMonthYear = lambda x: '{}-{}'.format(x.month, x.year)\n",
    "\n",
    "crimes = map(lambda x: x[0], catCount)\n",
    "\n",
    "datesCrimesRDD = (CrimeHeadlessRDD\n",
    "                  .map(lambda x: ( (parseMonthYear(x.Dates), x.Category), 1))\n",
    "                  .reduceByKey(lambda x,y: x+y)\n",
    "                  .map(lambda (x,y): (x[0], (x[1], y)))\n",
    "                  .groupByKey()\n",
    "                  .mapValues(dict)\n",
    "                  .cache()\n",
    "                 )\n",
    "\n",
    "print datesCrimesRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "assert datesCrimesRDD.take(1)[0][1][u'KIDNAPPING']==12,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O próximo passo consiste em calcular o total de pares Mes-Ano para ser possível o cálculo da média.\n",
    "\n",
    "#### Finalmente, criamos a RDD `fractionCrimesDateRDD` em que a chave é Mes-Ano e o valor é uma lista da fração de cada tipo de crime ocorridos naquele mês e ano. Para gerar essa lista vamos utilizar o *list comprehension* do Python de tal forma a calcular a fração para cada crime na variável `crimes`.\n",
    "\n",
    "#### Os dicionários em Python tem um método chamado `get()` que permite atribuir um valor padrão caso a chave não exista. Ex.: `dicionario.get( chave, 0.0)` retornará 0.0 caso a chave não exista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2-2007', [(u'KIDNAPPING', 0.0023561751423522483), (u'WEAPON LAWS', 0.009621048497938347), (u'SECONDARY CODES', 0.011977223640290595), (u'WARRANTS', 0.05811898684468879), (u'LOITERING', 0.0007853917141174161), (u'EMBEZZLEMENT', 0.0025525230708816024), (u'FRAUD', 0.0170822697820538), (u'DRIVING UNDER THE INFLUENCE', 0.0025525230708816024), (u'VEHICLE THEFT', 0.04162576084822305), (u'ROBBERY', 0.02650697035146279), (u'BURGLARY', 0.03769880227763597), (u'SUSPICIOUS OCC', 0.04064402120557628), (u'RECOVERED VEHICLE', 0.005890437855880621), (u'ASSAULT', 0.08953465540938543), (u'FORGERY/COUNTERFEITING', 0.015904182210877676), (u'BAD CHECKS', 0.0005890437855880621), (u'DRUNKENNESS', 0.004712350284704497), (u'GAMBLING', 0.00039269585705870805), (u'OTHER OFFENSES', 0.14863538189672099), (u'SUICIDE', 0.0005890437855880621), (u'ARSON', 0.0009817396426467701), (u'DRUG/NARCOTIC', 0.09895935597879442), (u'SEX OFFENSES NON FORCIBLE', 0.00039269585705870805), (u'PROSTITUTION', 0.014922442568230904), (u'VANDALISM', 0.0465344590614569), (u'NON-CRIMINAL', 0.09621048497938346), (u'LIQUOR LAWS', 0.0033379147849990185), (u'TRESPASS', 0.0098173964264677), (u'SEX OFFENSES FORCIBLE', 0.004516002356175142), (u'STOLEN PROPERTY', 0.003730610642057726), (u'BRIBERY', 0.0005890437855880621), (u'FAMILY OFFENSES', 0.0007853917141174161), (u'MISSING PERSON', 0.03023758099352052), (u'DISORDERLY CONDUCT', 0.004712350284704497), (u'RUNAWAY', 0.0025525230708816024), (u'LARCENY/THEFT', 0.1639505203220106)])]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "totalPerDateRDD = (CrimeHeadlessRDD\n",
    "                   .map(lambda x: (parseMonthYear(x.Dates), 1))\n",
    "                   .reduceByKey(lambda x,y: x+y)\n",
    "                  )\n",
    "\n",
    "fractionCrimesDateRDD = (datesCrimesRDD\n",
    "                         .join(totalPerDateRDD)\n",
    "                         .map(lambda (x,y) : (x, [(category, float((float(y[0].get(category,0.0)) / float(y[1])))) for category in y[0]]))\n",
    "                         .cache()\n",
    "                        )\n",
    "\n",
    "print fractionCrimesDateRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valores incorretos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-0cfa661a7f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfractionCrimesDateRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.163950\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'valores incorretos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'ok'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valores incorretos"
     ]
    }
   ],
   "source": [
    "assert np.abs(fractionCrimesDateRDD.take(1)[0][1][0][1]-0.163950)<1e-6,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, utilizaremos a função `Statistics.corr()` da biblioteca [`pyspark.mlllib.stat`](https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.stat.Statistics-class.html).\n",
    "\n",
    "#### Para isso mapeamos nossa RDD para conter apenas a lista de valores da lista de tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3673.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 464.0 failed 1 times, most recent failure: Lost task 0.0 in stage 464.0 (TID 1723, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 36 but got 35.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:67)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:334)\r\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\r\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\r\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:846)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 36 but got 35.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:67)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-27eac8ae0631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfractionCrimesDateRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pyspark\\mllib\\stat\\_statistics.pyc\u001b[0m in \u001b[0;36mcorr\u001b[1;34m(x, y, method)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3673.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 464.0 failed 1 times, most recent failure: Lost task 0.0 in stage 464.0 (TID 1723, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 36 but got 35.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:67)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:334)\r\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\r\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\r\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:846)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 36 but got 35.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\r\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:67)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$17.apply(RowMatrix.scala:420)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "corr = Statistics.corr(fractionCrimesDateRDD.map(lambda rec: map(lambda x: x[1],rec[1])))\n",
    "print corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo a matriz `corr` para `np.array` podemos buscar pelo maior valor negativo e positivo diferentes de 1.0. Para isso vamos utilizar as funções `min()` e `argmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npCorr = np.array(corr)\n",
    "rowMin = npCorr.min(axis=1).argmin()\n",
    "colMin = npCorr[rowMin,:].argmin()\n",
    "print crimes[rowMin], crimes[colMin], npCorr[rowMin,colMin]\n",
    "\n",
    "npCorr[npCorr==1.] = 0.\n",
    "rowMax = npCorr.max(axis=1).argmax()\n",
    "colMax = npCorr[rowMax,:].argmax()\n",
    "print crimes[rowMax], crimes[colMax], npCorr[rowMax,colMax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora que sabemos quais crimes tem maior correlação, vamos plotar um gráfico de dispersão daqueles com maior correlação negativa.\n",
    "\n",
    "#### Primeiro criamos duas RDDs, `var1RDD` e `var2RDD`. Elas são um mapeamento da `fractionCrimesDateRDD` filtradas para conter apenas o crime contido em Xlabel e Ylabel, respectivamente.\n",
    "\n",
    "#### Juntamos as duas RDDs em uma única RDD, `correlationRDD` que mapeará para tuplas de valores, onde os valores são as médias calculadas em fractionCrimesDateRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction\n",
      "[('2-2007', [(u'KIDNAPPING', 0.0023561751423522483), (u'WEAPON LAWS', 0.009621048497938347), (u'SECONDARY CODES', 0.011977223640290595), (u'WARRANTS', 0.05811898684468879), (u'LOITERING', 0.0007853917141174161), (u'EMBEZZLEMENT', 0.0025525230708816024), (u'FRAUD', 0.0170822697820538), (u'DRIVING UNDER THE INFLUENCE', 0.0025525230708816024), (u'VEHICLE THEFT', 0.04162576084822305), (u'ROBBERY', 0.02650697035146279), (u'BURGLARY', 0.03769880227763597), (u'SUSPICIOUS OCC', 0.04064402120557628), (u'RECOVERED VEHICLE', 0.005890437855880621), (u'ASSAULT', 0.08953465540938543), (u'FORGERY/COUNTERFEITING', 0.015904182210877676), (u'BAD CHECKS', 0.0005890437855880621), (u'DRUNKENNESS', 0.004712350284704497), (u'GAMBLING', 0.00039269585705870805), (u'OTHER OFFENSES', 0.14863538189672099), (u'SUICIDE', 0.0005890437855880621), (u'ARSON', 0.0009817396426467701), (u'DRUG/NARCOTIC', 0.09895935597879442), (u'SEX OFFENSES NON FORCIBLE', 0.00039269585705870805), (u'PROSTITUTION', 0.014922442568230904), (u'VANDALISM', 0.0465344590614569), (u'NON-CRIMINAL', 0.09621048497938346), (u'LIQUOR LAWS', 0.0033379147849990185), (u'TRESPASS', 0.0098173964264677), (u'SEX OFFENSES FORCIBLE', 0.004516002356175142), (u'STOLEN PROPERTY', 0.003730610642057726), (u'BRIBERY', 0.0005890437855880621), (u'FAMILY OFFENSES', 0.0007853917141174161), (u'MISSING PERSON', 0.03023758099352052), (u'DISORDERLY CONDUCT', 0.004712350284704497), (u'RUNAWAY', 0.0025525230708816024), (u'LARCENY/THEFT', 0.1639505203220106)]), ('7-2008', [(u'KIDNAPPING', 0.0023086485526549456), (u'WEAPON LAWS', 0.009234594210619783), (u'SECONDARY CODES', 0.008169064109394424), (u'WARRANTS', 0.04333155744983129), (u'LOITERING', 0.0015982951518380393), (u'EMBEZZLEMENT', 0.0007103534008169064), (u'FRAUD', 0.019001953471852247), (u'DRIVING UNDER THE INFLUENCE', 0.0028414136032676256), (u'SEX OFFENSES FORCIBLE', 0.004084532054697212), (u'ROBBERY', 0.02912448943349316), (u'BURGLARY', 0.038891848694725624), (u'STOLEN PROPERTY', 0.003906943704492985), (u'PORNOGRAPHY/OBSCENE MAT', 0.0001775883502042266), (u'FAMILY OFFENSES', 0.0005327650506126798), (u'FORGERY/COUNTERFEITING', 0.019179541822056473), (u'BAD CHECKS', 0.0007103534008169064), (u'SUSPICIOUS OCC', 0.03480731664002842), (u'ARSON', 0.001775883502042266), (u'GAMBLING', 0.0003551767004084532), (u'EXTORTION', 0.0001775883502042266), (u'DRUNKENNESS', 0.004794885455514118), (u'SUICIDE', 0.0012431184514295863), (u'RECOVERED VEHICLE', 0.006570768957556384), (u'DRUG/NARCOTIC', 0.09128041200497247), (u'TRESPASS', 0.008169064109394424), (u'PROSTITUTION', 0.01775883502042266), (u'VANDALISM', 0.044752264251465106), (u'NON-CRIMINAL', 0.08453205469721187), (u'LIQUOR LAWS', 0.0033741786538803056), (u'VEHICLE THEFT', 0.03693837684247913), (u'OTHER OFFENSES', 0.176345231752797), (u'ASSAULT', 0.08044752264251465), (u'MISSING PERSON', 0.02770378263185935), (u'DISORDERLY CONDUCT', 0.007458710708577517), (u'RUNAWAY', 0.002131060202450719), (u'LARCENY/THEFT', 0.1855798259634168)])]\n",
      "var1RDD\n",
      "[('2-2007', 0.015904182210877676), ('7-2008', 0.019179541822056473)]\n",
      "var2RDD\n",
      "[('2-2007', 0.09621048497938346), ('7-2008', 0.08453205469721187)]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 507.0 failed 1 times, most recent failure: Lost task 2.0 in stage 507.0 (TID 1748, localhost, executor driver): org.apache.spark.SparkException: Block rdd_722_2 was not found even though it's read-locked\r\n\tat org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:489)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:539)\r\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:693)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:753)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Block rdd_722_2 was not found even though it's read-locked\r\n\tat org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:489)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:539)\r\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:693)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:753)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-43ac893bb9f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrelationRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 507.0 failed 1 times, most recent failure: Lost task 2.0 in stage 507.0 (TID 1748, localhost, executor driver): org.apache.spark.SparkException: Block rdd_722_2 was not found even though it's read-locked\r\n\tat org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:489)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:539)\r\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:693)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:753)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Block rdd_722_2 was not found even though it's read-locked\r\n\tat org.apache.spark.storage.BlockManager.handleLocalReadFailure(BlockManager.scala:489)\r\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:539)\r\n\tat org.apache.spark.storage.BlockManager.get(BlockManager.scala:693)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:753)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)\r\n\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:333)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\r\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\r\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "Xlabel = 'FORGERY/COUNTERFEITING'#'DRIVING UNDER THE INFLUENCE'\n",
    "Ylabel = 'NON-CRIMINAL'#'LIQUOR LAWS'\n",
    "\n",
    "print \"fraction\"\n",
    "print fractionCrimesDateRDD.take(2)\n",
    "\n",
    "\n",
    "var1RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Xlabel,rec[1])[0][1]))\n",
    "          )\n",
    "\n",
    "print \"var1RDD\"\n",
    "print var1RDD.take(2)\n",
    "\n",
    "var2RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Ylabel,rec[1])[0][1]))\n",
    "          )\n",
    "\n",
    "print \"var2RDD\"\n",
    "print var2RDD.take(2)\n",
    "\n",
    "correlationRDD = (var1RDD\n",
    "                  .join(var2RDD)\n",
    "                  #.<COMPLETAR>\n",
    "                 )\n",
    "\n",
    "\n",
    "Data = correlationRDD.collect()\n",
    "print Data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.abs(Data[0][0]-0.015904)<1e-6, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, é possível perceber que quanto mais crimes do tipo *NON-CRIMINAL* ocorrem em um dia, menos *FORGERY/COUNTERFEITING* ocorrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = zip(*Data)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.scatter(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel(Xlabel)\n",
    "plt.ylabel(Ylabel)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Histograma **\n",
    "\n",
    "#### O uso do Histograma é para visualizar a distribuição dos dados. Dois tipos de distribuição que são observadas normalmente é a Gaussiana, em que os valores se concentram em torno de uma média e a Lei de Potência, em que os valores menores são observados com maior frequência.\n",
    "\n",
    "#### Vamos verificar a distribuição das prisões efetuadas (categoria *ARREST* em * Resolution*) em cada mês. Com essa distribuição poderemos verificar se o número de prisões é consistente durante os meses do período estudado.\n",
    "\n",
    "#### Primeiro criaremos uma RDD chamada `bookedRDD` que contém apenas os registros contendo *ARREST* no campo *Resolution* (lembre-se que esse campo é uma lista) e contabilizar a quantidade de registros em cada 'Mes-Ano'. Ao final, vamos mapear para uma RDD contendo apenas os valores contabilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "bookedRDD = (CrimeHeadlessRDD\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "             .<COMPLETAR>\n",
    "            )\n",
    "\n",
    "Data = bookedRDD.collect()\n",
    "print Data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert Data[0]==1914,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.hist(Data)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel('ARRESTED')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notem que lemos o histograma da seguinte maneira: em cerca de 50 meses foram observadas entre 1750 e 2000 prisões. Porém, não sabemos precisar em quais meses houve um aumento ou redução das prisões. Isso deve ser observado através de um gráfico de linha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) Box-plot**\n",
    "\n",
    "#### O Box-plot é um gráfico muito utilizado em estatística para visualizar o resumo estatístico de uma variável.\n",
    "\n",
    "#### Para esse exercício vamos plotar duas box-plot sobre a média do número de prisões durante os meses analisados para os crimes do tipo *ROBBERY* e *ASSAULT*.\n",
    "\n",
    "#### O mapeamento é exatamente o mesmo do exercício anterior, porém filtrando para o tipo de roubo analisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseDayMonth = lambda x: '{}-{}'.format(x.month,x.year)\n",
    "\n",
    "robberyBookedRDD = (CrimeHeadlessRDD\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                    )\n",
    "\n",
    "assaultBookedRDD = (CrimeHeadlessRDD\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                     .<COMPLETAR>\n",
    "                    )\n",
    "\n",
    "robData = robberyBookedRDD.collect()\n",
    "assData = assaultBookedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert robData[0]==27,'valores incorretos'\n",
    "print 'ok'\n",
    "assert assData[0]==152,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, percebemos que existem, em média, muito mais prisões para o tipo *ASSAULT* do que o tipo *ROBBERY*, ambos com pequena variação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.boxplot([robData,assData])\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('ARRESTED')\n",
    "plt.xticks([1,2], ['ROBBERY','ASSAULT'])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
